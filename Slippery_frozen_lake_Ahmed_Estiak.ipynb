{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc540ab-0e35-4999-b62f-d4a1f8e4d068",
   "metadata": {},
   "source": [
    "## Q-table in the case of Frozen Lake (Slippery is True) problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42893926-d7a3-4245-8c47-8e21483b3f83",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gymnasium'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgymnasium\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgym\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gymnasium'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True, render_mode=\"ansi\")\n",
    "env.reset()\n",
    "action_size = env.action_space.n\n",
    "print(\"Action size: \", action_size)\n",
    "state_size = env.observation_space.n\n",
    "print(\"State size: \", state_size)\n",
    "print(env.render())\n",
    "\n",
    "\n",
    "def eval_policy_better(env_, pi_, gamma_, t_max_, episodes_):\n",
    "    \n",
    "    goal_reached_number = 0 #to_count_how_many_times_the_agent_reached_the_goal\n",
    "    \n",
    "    v_pi_rep = np.empty(episodes_) # N trials\n",
    "    for e in range(episodes_):\n",
    "        s_t = env.reset()[0]\n",
    "        v_pi = 0\n",
    "        for t in range(t_max_):\n",
    "            a_t = pi_[s_t]\n",
    "            s_t, r_t, done, truncated, info = env_.step(a_t) \n",
    "            v_pi += gamma_**t*r_t\n",
    "            if done:\n",
    "                if r_t == 1:\n",
    "                    goal_reached_number = goal_reached_number+1\n",
    "                break\n",
    "        v_pi_rep[e] = v_pi\n",
    "        env.close()\n",
    "    print(f'The agent reached its goal {goal_reached_number} times.')\n",
    "    return np.mean(v_pi_rep), np.min(v_pi_rep), np.max(v_pi_rep), np.std(v_pi_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "822d6023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo Legion\\Desktop\\intro_ml\\mnist_noise\\.venv\\Scripts\\python.exe\n",
      "['c:\\\\Users\\\\Lenovo Legion\\\\Desktop\\\\intro_ml\\\\mnist_noise\\\\.venv', 'c:\\\\Users\\\\Lenovo Legion\\\\Desktop\\\\intro_ml\\\\mnist_noise\\\\.venv\\\\Lib\\\\site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys; print(sys.executable); import site; print(site.getsitepackages())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b062ed26-1022-442a-bdf4-a74d715c41a0",
   "metadata": {},
   "source": [
    "Q-table by Mihail. I have taken the 100000 episodes so that the Value function mean and reached_goals_number don't deviate much in every run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a6dfc2c-bca4-4bd1-bc5d-24536d76b83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[0 3 3 3 0 0 1 0 2 1 0 0 0 2 1 0]\n",
      "The agent reached its goal 21903 times.\n",
      "Value function mean 0.0380, min 0.0000 max 0.5905 and std 0.0998\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "Q = np.zeros([state_size, action_size])\n",
    "Q[0,0] = 1\n",
    "Q[1,3] = 1\n",
    "Q[2,3] = 1\n",
    "Q[3,3] = 1\n",
    "Q[4,0] = 1\n",
    "Q[6,1] = 1\n",
    "Q[8,2] = 1\n",
    "Q[9,1] = 1\n",
    "Q[10,0] = 1\n",
    "Q[13,2] = 1\n",
    "Q[14,1] = 1 \n",
    "\n",
    "print(Q)\n",
    "\n",
    "pi_Q = np.argmax(Q,axis=1)\n",
    "print(pi_Q)\n",
    "val_mean, val_min, val_max, val_std = eval_policy_better(env, pi_Q, gamma, 666, 100000)\n",
    "print(f'Value function mean {val_mean:.4f}, min {val_min:.4f} max {val_max:.4f} and std {val_std:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3559ac-9e6a-4d0b-a382-eabafd5bba32",
   "metadata": {},
   "source": [
    "Well, the Value function mean is 0.0380, and the agent reached the goal 21903 times successfully. That means only 21.86% times it raches its goal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cefdf27-802b-4e90-844a-971ed08094f7",
   "metadata": {},
   "source": [
    "I will analyze two separate Q-tables that I have deduced and compare their results later. First, I will change the actions in state 6 and state 8 in Mihail's Q-table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "053d7c9a-27d8-44a7-bb50-62ee2f603389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[0 3 3 3 0 0 2 0 3 1 0 0 0 2 1 0]\n",
      "The agent reached its goal 82234 times.\n",
      "Value function mean 0.0679, min 0.0000 max 0.5905 and std 0.1129\n"
     ]
    }
   ],
   "source": [
    "Q = np.zeros([state_size, action_size])\n",
    "Q[0,0] = 1\n",
    "Q[1,3] = 1\n",
    "Q[2,3] = 1\n",
    "Q[3,3] = 1\n",
    "Q[4,0] = 1\n",
    "Q[6,2] = 1   #I chose action 2 (go right which means towards the lake instead of going down)\n",
    "Q[8,3] = 1   #I chose action 3 (go up which instead of going right), this action is the major development in this Q-table\n",
    "Q[9,1] = 1\n",
    "Q[10,0] = 1\n",
    "Q[13,2] = 1\n",
    "Q[14,1] = 1 \n",
    "\n",
    "print(Q)\n",
    "\n",
    "pi_Q = np.argmax(Q,axis=1)\n",
    "print(pi_Q)\n",
    "val_mean, val_min, val_max, val_std = eval_policy_better(env, pi_Q, gamma, 666, 100000)\n",
    "print(f'Value function mean {val_mean:.4f}, min {val_min:.4f} max {val_max:.4f} and std {val_std:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f43890-25e2-41d8-b75e-11e8ad0073b1",
   "metadata": {},
   "source": [
    "Aha! The Value function mean is now 0.0679 (much better), and the agent successfully reached the goal 82234 times (3.75 times than the previous one). That means 82.25% times it raches its goal. A good development. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15694e1-46eb-445a-9cb8-5842d9401aa7",
   "metadata": {},
   "source": [
    "In this step, I chose to jump into the lake instead of trying to avoid it. Which is probabilistically more preferred anyway (more in my PDF). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0a677d-ca99-4785-91fb-3c0ad96b358c",
   "metadata": {},
   "source": [
    "**Thatâ€™s life**; It's true that you need to go towards your goal, avoiding problems. Sometimes you need to do some crazy stuff like jumping upward or toward the wall. But now we know, we need to take risks also, like jumping to the lake where death is inevitable. It seems scary. Your brain will try to stop this action. But you need to take risks to do better in your life. But my suggestion is to take calculated risk just like my Qtable.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8fdf1d-6095-4bee-8ca6-5825436f3d14",
   "metadata": {},
   "source": [
    "Now, I will change the action in state 2 in my latest Q-table and will observe what will happen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36e7d1c1-f640-427d-a96d-e04dd1d52896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[0 3 1 3 0 0 2 0 3 1 0 0 0 2 1 0]\n",
      "The agent reached its goal 78281 times.\n",
      "Value function mean 0.0693, min 0.0000 max 0.5905 and std 0.1142\n"
     ]
    }
   ],
   "source": [
    "Q = np.zeros([state_size, action_size])\n",
    "Q[0,0] = 1\n",
    "Q[1,3] = 1\n",
    "Q[2,1] = 1\n",
    "Q[3,3] = 1\n",
    "Q[4,0] = 1\n",
    "Q[6,2] = 1\n",
    "Q[8,3] = 1\n",
    "Q[9,1] = 1\n",
    "Q[10,0] = 1\n",
    "Q[13,2] = 1\n",
    "Q[14,1] = 1 \n",
    "\n",
    "print(Q)\n",
    "\n",
    "pi_Q = np.argmax(Q,axis=1)\n",
    "print(pi_Q)\n",
    "val_mean, val_min, val_max, val_std = eval_policy_better(env, pi_Q, gamma, 666, 100000)\n",
    "print(f'Value function mean {val_mean:.4f}, min {val_min:.4f} max {val_max:.4f} and std {val_std:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb7a6e9-b5ec-424e-8a4b-e85902909604",
   "metadata": {},
   "source": [
    "Look! The Value function mean is now 0.0693 (slightly better total reward points achieved than my previous Q-table). However, the agent successfully reached the goal 78281 times (less than the previous one). So, the previous Qtable was better for reaching the goal successfully, but this one is better in the case of collecting rewards. I will discuss all these issues more in my PDF (Observations and reasoning behind my Q-table in the case of Frozen Lake (Slippery is True) problem)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
