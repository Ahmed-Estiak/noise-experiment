{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03faea4d-6100-417a-8fe3-1c9ebb50eb8c",
   "metadata": {},
   "source": [
    "## MNIST original (Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f0a4f65-01f5-4894-aa7f-f6ba2c380532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy for original MNIST data with no noise in test data (Naive Bayes): 81.50%\n",
      "Classification accuracy for original MNIST data with addesd noise in test data (Naive Bayes): 84.68%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape the training and test sets\n",
    "x_train = x_train.reshape(x_train.shape[0], 28*28).astype(np.float32)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28*28).astype(np.float32)\n",
    "\n",
    "# Normalize the pixel values\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0\n",
    "\n",
    "number_classes = 10\n",
    "number_features = x_train.shape[1]  \n",
    "\n",
    "\n",
    "means = np.zeros((number_classes, number_features))\n",
    "variances = np.zeros((number_classes, number_features))\n",
    "\n",
    "def compute_log_likelihood(x, class_index):\n",
    "    mean = means[class_index]\n",
    "    variance = variances[class_index]\n",
    "    log_likelihood = -0.5 * (np.sum(np.log(2 * np.pi * variance)) + np.sum(((x - mean) ** 2) / variance))\n",
    "    return log_likelihood\n",
    "\n",
    "#evaluation function\n",
    "def class_acc(pred, gt):\n",
    "    pred = np.array(pred)\n",
    "    gt = np.array(gt)\n",
    "    \n",
    "    correct_predictions = np.sum(pred == gt)\n",
    "    accuracy = (correct_predictions / len(gt)) * 100\n",
    "    \n",
    "    return accuracy\n",
    "np.random.seed(0)\n",
    "noise1 = np.random.normal(loc=0.0, scale=0.14, size=x_train.shape) #added noise with train data\n",
    "x_train_noise = x_train + noise1\n",
    "\n",
    "noise2 = np.random.normal(loc=0.0, scale=0.19, size=x_test.shape) #aaded noise with test data\n",
    "x_test_noise = x_test + noise2\n",
    "\n",
    "y_pred = []\n",
    "\n",
    "for k in range(number_classes):\n",
    "    class_k_samples = x_train_noise[y_train == k]\n",
    "    means[k, :] = np.mean(class_k_samples, axis=0)\n",
    "    variances[k, :] = np.var(class_k_samples, axis=0)\n",
    "    \n",
    "for i in range(x_test.shape[0]):\n",
    "    log_likelihoods = np.array([compute_log_likelihood(x_test[i], k) for k in range(number_classes)])\n",
    "    predicted_class = np.argmax(log_likelihoods) \n",
    "    y_pred.append(predicted_class)\n",
    "accuracy = class_acc(y_pred, y_test)\n",
    "\n",
    "print(f'Classification accuracy for original MNIST data with no noise in test data (Naive Bayes): {accuracy:.2f}%')  #np.random.seed(0) for noise\n",
    "\n",
    "y_test_noise_pred = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    log_likelihoods = np.array([compute_log_likelihood(x_test_noise[i], k) for k in range(number_classes)])\n",
    "    predicted_class = np.argmax(log_likelihoods) \n",
    "    y_test_noise_pred.append(predicted_class)\n",
    "accuracy_test_noise= class_acc(y_test_noise_pred, y_test)\n",
    "\n",
    "print(f'Classification accuracy for original MNIST data with addesd noise in test data (Naive Bayes): {accuracy_test_noise:.2f}%')  #np.random.seed(0) for noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87295905-a767-41c3-beb0-30603f887166",
   "metadata": {},
   "source": [
    " **So, after adding noise in the test data the accuracy is 3.18% better in this case**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e62ab66-3f7e-47e0-b5dd-3fc2a6a03901",
   "metadata": {},
   "source": [
    "## MNIST Fashion (Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02ca3282-e4e9-44d4-b2fc-791dec39a224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy for fashion MNIST data with no noise in test data (Naive Bayes): 67.34%\n",
      "Classification accuracy for fashion MNIST data with addesd noise in test data (Naive Bayes): 71.77%\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape the training and test sets\n",
    "x_train = x_train.reshape(x_train.shape[0], 28*28).astype(np.float32)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28*28).astype(np.float32)\n",
    "\n",
    "# Normalize the pixel values\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0\n",
    "\n",
    "number_classes = 10\n",
    "number_features = x_train.shape[1]  \n",
    "\n",
    "\n",
    "means = np.zeros((number_classes, number_features))\n",
    "variances = np.zeros((number_classes, number_features))\n",
    "\n",
    "def compute_log_likelihood(x, class_index):\n",
    "    mean = means[class_index]\n",
    "    variance = variances[class_index]\n",
    "    log_likelihood = -0.5 * (np.sum(np.log(2 * np.pi * variance)) + np.sum(((x - mean) ** 2) / variance))\n",
    "    return log_likelihood\n",
    "\n",
    "#evaluation function\n",
    "def class_acc(pred, gt):\n",
    "    pred = np.array(pred)\n",
    "    gt = np.array(gt)\n",
    "    # \n",
    "    correct_predictions = np.sum(pred == gt)\n",
    "    accuracy = (correct_predictions / len(gt)) * 100\n",
    "    \n",
    "    return accuracy\n",
    "np.random.seed(0)\n",
    "noise1 = np.random.normal(loc=0.0, scale=0.06, size=x_train.shape)\n",
    "x_train_noise = x_train + noise1\n",
    "\n",
    "noise2 = np.random.normal(loc=0.0, scale=0.1, size=x_test.shape)\n",
    "x_test_noise = x_test + noise2\n",
    "\n",
    "y_pred = []\n",
    "\n",
    "for k in range(number_classes):\n",
    "    class_k_samples = x_train_noise[y_train == k]\n",
    "    means[k, :] = np.mean(class_k_samples, axis=0)\n",
    "    variances[k, :] = np.var(class_k_samples, axis=0)\n",
    "\n",
    "for i in range(x_test.shape[0]):\n",
    "    log_likelihoods = np.array([compute_log_likelihood(x_test[i], k) for k in range(number_classes)])\n",
    "    predicted_class = np.argmax(log_likelihoods) \n",
    "    y_pred.append(predicted_class)\n",
    "accuracy = class_acc(y_pred, y_test)\n",
    "print(f'Classification accuracy for fashion MNIST data with no noise in test data (Naive Bayes): {accuracy:.2f}%') #np.random.seed(0) for noise\n",
    "\n",
    "y_test_noise_pred = []\n",
    "\n",
    "for i in range(x_test.shape[0]):\n",
    "    log_likelihoods = np.array([compute_log_likelihood(x_test_noise[i], k) for k in range(number_classes)])\n",
    "    predicted_class = np.argmax(log_likelihoods) \n",
    "    y_test_noise_pred.append(predicted_class)\n",
    " \n",
    "accuracy_test_noise= class_acc(y_test_noise_pred, y_test)\n",
    "print(f'Classification accuracy for fashion MNIST data with addesd noise in test data (Naive Bayes): {accuracy_test_noise:.2f}%') #np.random.seed(0) for noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a839c6df-1f7a-4936-90c7-998db4369d66",
   "metadata": {},
   "source": [
    "**So, after adding noise in the test data the accuracy is 4.43% better in this case.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70188370-5964-4658-96c3-96a5785be05f",
   "metadata": {},
   "source": [
    "## MNIST original (Full Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7d277ac-6d41-4243-a83f-d9d1bcf9fc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy for original MNIST data with no noise in test data (Full Bayes): 95.87%\n",
      "Classification accuracy for original MNIST data with addesd noise in test data (Full Bayes): 95.94%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape the training and test sets\n",
    "x_train = x_train.reshape(x_train.shape[0], 28*28).astype(np.float32)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28*28).astype(np.float32)\n",
    "\n",
    "# Normalize the pixel values\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0\n",
    "\n",
    "# Function to compute classification accuracy\n",
    "def class_acc(pred, gt):\n",
    "    pred = np.array(pred)\n",
    "    gt = np.array(gt)\n",
    "    correct_predictions = np.sum(pred == gt)\n",
    "    accuracy = (correct_predictions / len(gt)) * 100\n",
    "    return accuracy\n",
    "\n",
    "# Function to calculate log likelihoods and predictions\n",
    "def calculate_predictions(x_train_noise, x_test, y_train):\n",
    "    number_classes = 10\n",
    "    number_features = x_train_noise.shape[1]\n",
    "\n",
    "    means = np.zeros((number_classes, number_features))\n",
    "    covariances = np.zeros((number_classes, number_features, number_features))\n",
    "\n",
    "    # Compute mean and covariance for each class\n",
    "    for k in range(number_classes):\n",
    "        class_k_samples = x_train_noise[y_train == k]\n",
    "        means[k, :] = np.mean(class_k_samples, axis=0)\n",
    "        covariances[k, :, :] = np.cov(class_k_samples.T)\n",
    "\n",
    "    log_likelihoods = np.zeros((x_test.shape[0], number_classes))\n",
    "\n",
    "    for k in range(number_classes):\n",
    "        log_likelihoods[:, k] = multivariate_normal.logpdf(x_test, mean=means[k], cov=covariances[k])\n",
    "\n",
    "    predicted_classes = np.argmax(log_likelihoods, axis=1)\n",
    "    return predicted_classes\n",
    "np.random.seed(0)\n",
    "noise1 = np.random.normal(loc=0.0, scale=0.27, size=x_train.shape)\n",
    "x_train_noise = x_train + noise1\n",
    "\n",
    "noise2 = np.random.normal(loc=0.0, scale=0.06, size=x_test.shape)\n",
    "x_test_noise=x_test + noise2\n",
    "\n",
    "# For original MNIST, no noise in test data\n",
    "predicted_classes = calculate_predictions(x_train_noise, x_test, y_train)\n",
    "# For original MNIST, added noise in test data\n",
    "predicted_classes_test_noise = calculate_predictions(x_train_noise, x_test_noise, y_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = class_acc(predicted_classes, y_test)\n",
    "accuracy_test_noise= class_acc(predicted_classes_test_noise, y_test)\n",
    "\n",
    "print(f'Classification accuracy for original MNIST data with no noise in test data (Full Bayes): {accuracy:.2f}%')  #np.random.seed(0) for noise\n",
    "print(f'Classification accuracy for original MNIST data with addesd noise in test data (Full Bayes): {accuracy_test_noise:.2f}%')  #np.random.seed(0) for noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d860e2-fa12-4b4b-870f-1a3a2d20193f",
   "metadata": {},
   "source": [
    "**So, after adding noise in the test data the accuracy is 0.07% better in this case.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a75110-a37c-4a46-b1d9-9cc11706d327",
   "metadata": {},
   "source": [
    "## MNIST Fashion (Full Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e53b65fe-6c7d-483d-ba1a-2591f6c19f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy for fashion MNIST data with no noise in test data (Full Bayes): 77.47%\n",
      "Classification accuracy for fashion MNIST data with addesd noise in test data (Full Bayes): 82.64%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape the training and test sets\n",
    "x_train = x_train.reshape(x_train.shape[0], 28*28).astype(np.float32)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28*28).astype(np.float32)\n",
    "\n",
    "# Normalize the pixel values\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0\n",
    "\n",
    "# Function to compute classification accuracy\n",
    "def class_acc(pred, gt):\n",
    "    pred = np.array(pred)\n",
    "    gt = np.array(gt)\n",
    "    correct_predictions = np.sum(pred == gt)\n",
    "    accuracy = (correct_predictions / len(gt)) * 100\n",
    "    return accuracy\n",
    "\n",
    "# Function to calculate log likelihoods and predictions\n",
    "def calculate_predictions(x_train_noise, x_test, y_train):\n",
    "    number_classes = 10\n",
    "    number_features = x_train_noise.shape[1]\n",
    "\n",
    "    means = np.zeros((number_classes, number_features))\n",
    "    covariances = np.zeros((number_classes, number_features, number_features))\n",
    "\n",
    "    # Compute mean and covariance for each class\n",
    "    for k in range(number_classes):\n",
    "        class_k_samples = x_train_noise[y_train == k]\n",
    "        means[k, :] = np.mean(class_k_samples, axis=0)\n",
    "        covariances[k, :, :] = np.cov(class_k_samples.T)\n",
    "\n",
    "    log_likelihoods = np.zeros((x_test.shape[0], number_classes))\n",
    "\n",
    "    for k in range(number_classes):\n",
    "        log_likelihoods[:, k] = multivariate_normal.logpdf(x_test, mean=means[k], cov=covariances[k])\n",
    "\n",
    "    predicted_classes = np.argmax(log_likelihoods, axis=1)\n",
    "    return predicted_classes\n",
    "np.random.seed(0)\n",
    "noise3 = np.random.normal(loc=0.0, scale=0.15, size=x_train.shape)\n",
    "x_train_noise = x_train + noise3\n",
    "\n",
    "noise4 = np.random.normal(loc=0.0, scale=0.14, size=x_test.shape)\n",
    "x_test_noise=x_test + noise4\n",
    "\n",
    "# For fashion MNIST, no noise in test data\n",
    "predicted_classes = calculate_predictions(x_train_noise, x_test, y_train)\n",
    "# For fashion MNIST, added noise in test data\n",
    "predicted_classes_test_noise = calculate_predictions(x_train_noise, x_test_noise, y_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = class_acc(predicted_classes, y_test)\n",
    "accuracy_test_noise= class_acc(predicted_classes_test_noise, y_test)\n",
    "\n",
    "print(f'Classification accuracy for fashion MNIST data with no noise in test data (Full Bayes): {accuracy:.2f}%')  #np.random.seed(0) for noise\n",
    "print(f'Classification accuracy for fashion MNIST data with addesd noise in test data (Full Bayes): {accuracy_test_noise:.2f}%')  #np.random.seed(0) for noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f59e29-35e3-452e-b907-96c010c57f86",
   "metadata": {},
   "source": [
    "**So, after adding noise in the test data the accuracy is 5.17% better in this case.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
